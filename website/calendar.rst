Autumn 2018 Calendar and Reading List
-------------------------------------

----

Week 1: Motivation and Introduction
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, October 1**: Introduction to Deep Learning Models

Here are `the introductory slides <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture1-Intro-to-Deep-Learning-Part1.pdf>`_ (download).

Important things to do:

- Sign up for `Google Collaboratory <https://colab.research.google.com>`_, which we will use to run examples. 
- Read Chapters 1 and 2 of `Deep Learning with Python <http://www.deeplearningitalia.com/wp-content/uploads/2017/12/Dropbox_Chollet.pdf>`_ by Francois Chollet.
- Work through the Python code examples in Chapter 2 of Chollet. Those in Section 2.1 are to be found `here <https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/2.1-a-first-look-at-a-neural-network.ipynb>`_. (From within Google Collaboratory, select File then Upload notebook, then Git, and give the URL `https://github.com/fchollet/deep-learning-with-python-notebooks.git <https://github.com/fchollet/deep-learning-with-python-notebooks.git>`_.) Those in subsequent subsections you'll need to copy and paste.

Other useful links:

* `Arxiv Sanity Preserver <http://www.arxiv-sanity.com>`_
* `Alena Kruchkova videos <https://www.youtube.com/channel/UCF9O8Vj-FEbRDA5DcDGz-Pg>`_
* The `sample code and data referred to in class <https://drive.google.com/drive/folders/1-jkm2bUYWOftKm8is6rx3dKP9UIz2hCC?usp=sharing>`_

**Wednesday, October 3**: Functional Content of Deep Learning Frameworks


- Here are `the second set of introductory slides <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture2-Intro-to-Deep-Learning-Part2.pdf>`_ (download) and `the slides describing running Keras and material relevant to the first assignment <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture2-RunningKeras.pdf>`_ (download).

- Read `Tensorflow: a system for large-scale machine learning <https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf>`_, Abadi, Mart√≠n, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin et al. OSDI, vol. 16, pp. 265-283. 2016.

*Suggested Reading*.
Note: Some of these readings go into topics that we will cover later in the quarter.
As such, you may not get that much out of reading these references at the start
of the quarter. Instead, they can be good reference material to re-read later on
and see how everything fits together.

- `Deep Learning <https://www.deeplearningbook.org>`_, Ian Goodfellow, Yoshua Bengio, Aaron Courville. MIT Press and online. 
- `Neural Networks and Deep Learning <http://neuralnetworksanddeeplearning.com>`_, Michael Neilsen. A particularly nice presentation of back propagation.
- `Cloud Computing for Science and Engineering <https://cloud4scieng.org>`_, Ian Foster and Dennis Gannon. MIT Press and online. Describes how to use deep learning services provided by Amazon, Google, and Microsoft.


----

Week 2 - Deep Learning Frameworks
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, October 8**: Keras

- `The slides on Keras <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture3-Keras.pdf>`_ (download).

- `Keras <https://keras.io>`_ (and the Chollet book referenced above)



**Wednesday, October 10**: Tensorflow

- `The slides on TensorFlow <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture4-TensorFlow.pdf>`_ (download).

- `TensorFlow <https://www.tensorflow.org>`_


----

Week 3 - Back Propagation and Training 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, October 15**: `Activation, Loss, and Back Propagation <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture5-Activation-Loss.pdf>`_ (slides download).

**Wednesday, October 17**: `Training Examples <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture5-TrainingExamples.pdf>`_ and `More Back Propagation <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture6-BackProp.pdf>`_ (slide downloads).


Papers to read:

- `DAWNBench: An End-to-End Deep Learning Benchmark and Competition <https://dawn.cs.stanford.edu/benchmark/papers/nips17-dawnbench.pdf>`_, Cody Coleman et al., NIPS 2017. Also `DAWNBench web site <https://dawn.cs.stanford.edu/benchmark/>`_ and `Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark <https://arxiv.org/pdf/1806.01427.pdf>`_, Cody Coleman et al., Arxiv.
- `MLPerf <https://mlperf.org>`_

----

Week 4 - TOPIC TBD
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, October 22**

- TBD

**Wednesday, October 24**

- TBD 

----

Week 5 - TOPIC TBD
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, October 29**

- Guest lecture: TBD.

**Wednesday, October 31**

- TBD

----

Week 6 - TOPIC TBD
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, November 5**

- Guest lecture: TBD.

**Wednesday, November 7**

- Guest lecture: TBD.

----

Week 7 - TOPIC TBD
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, November 12**

- Guest lecture: TBD.

**Wednesday, November 14**

- Guest lecture: TBD.


----

Week 8 - TOPIC TBD
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, November 19**

- TBD

**Wednesday, November 21**

- TBD

----

Week 9 - TOPIC TBD
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, November 26**

- TBD

**Wednesday, November 28**

- TBD


----

Week 10 - Project presentations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, December 3**

- Project presentations

**Wednesday, December 5**

- Project presentations

----

Specific topics to be covered:

* Introduction to deep learning models
* Functional content of deep learning frameworks
* Software architecture and design of frameworks
* Abstraction layers for deep learning
* Performance and benchmarking deep learning systems
* Hardware architectures for accelerating deep learning. E.g., `Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks <http://cadlab.cs.ucla.edu/~cong/slides/fpga2015_chen.pdf>`_, Chen Zhang et al,
* Parallelism (model, data, ensemble). E>g., `Large Scale Distributed Deep Networks <http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf>`_, Jeff Dean et al., NIPS 2012.
* Portable representations and translations of models
* Optimization for training, inference
* Workflows for machine learning and workflow tools
* Hyper-parameter optimization and ensembles
* Uncertainty quantification

Other potential topics:

* Compression of networks. E.g., `Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <https://arxiv.org/pdf/1510.00149.pdf>`_, Song Han et al, ICLR 2016.
* I/O

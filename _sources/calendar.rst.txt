Autumn 2018 Calendar and Reading List
-------------------------------------

----

Week 1: Motivation and Introduction
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, October 1**: Introduction to Deep Learning Models

Here are `the introductory slides <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture1-Intro-to-Deep-Learning-Part1.pdf>`_ (download).

Important things to do:

- Sign up for `Google Collaboratory <https://colab.research.google.com>`_, which we will use to run examples. 
- Read Chapters 1 and 2 of `Deep Learning with Python <http://www.deeplearningitalia.com/wp-content/uploads/2017/12/Dropbox_Chollet.pdf>`_ by Francois Chollet.
- Work through the Python code examples in Chapter 2 of Chollet. Those in Section 2.1 are to be found `here <https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/2.1-a-first-look-at-a-neural-network.ipynb>`_. (From within Google Collaboratory, select File then Upload notebook, then Git, and give the URL `https://github.com/fchollet/deep-learning-with-python-notebooks.git <https://github.com/fchollet/deep-learning-with-python-notebooks.git>`_.) Those in subsequent subsections you'll need to copy and paste.

Other useful links:

* `Arxiv Sanity Preserver <http://www.arxiv-sanity.com>`_
* `Alena Kruchkova videos <https://www.youtube.com/channel/UCF9O8Vj-FEbRDA5DcDGz-Pg>`_
* The `sample code and data referred to in class <https://drive.google.com/drive/folders/1-jkm2bUYWOftKm8is6rx3dKP9UIz2hCC?usp=sharing>`_

**Wednesday, October 3**: Functional Content of Deep Learning Frameworks


- Here are `the second set of introductory slides <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture2-Intro-to-Deep-Learning-Part2.pdf>`_ (download) and `the slides describing running Keras and material relevant to the first assignment <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture2-RunningKeras.pdf>`_ (download).

*Suggested Reading*.
Note: Some of these readings go into topics that we will cover later in the quarter.
As such, you may not get that much out of reading these references at the start
of the quarter. Instead, they can be good reference material to re-read later on
and see how everything fits together.

- `Deep Learning <https://www.deeplearningbook.org>`_, Ian Goodfellow, Yoshua Bengio, Aaron Courville. MIT Press and online. 
- `Neural Networks and Deep Learning <http://neuralnetworksanddeeplearning.com>`_, Michael Neilsen. A particularly nice presentation of back propagation.
- `Cloud Computing for Science and Engineering <https://cloud4scieng.org>`_, Ian Foster and Dennis Gannon. MIT Press and online. Describes how to use deep learning services provided by Amazon, Google, and Microsoft.


----

Week 2 - Deep Learning Frameworks
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, October 8**: Keras

- `The slides on Keras <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture3-Keras.pdf>`_ (slides download).

- `Keras <https://keras.io>`_ (and the Chollet book referenced above)



**Wednesday, October 10**: Tensorflow

- `The slides on TensorFlow <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture4-TensorFlow.pdf>`_ (slides download).

- `TensorFlow <https://www.tensorflow.org>`_


----

Week 3 - Back Propagation and Training 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, October 15**: `Activation, Loss, and Back Propagation <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture5-Activation-Loss.pdf>`_ (slides download).

**Wednesday, October 17**: `Training Examples <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture5-TrainingExamples.pdf>`_ and `More Back Propagation <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture6-BackProp.pdf>`_ (slides downloads).



----

Week 4 - Performance and Parallelism
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, October 22**: `TensorFlow Internals and Parallelism <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture7-Parallelism.pdf>`_ (slides download).

Papers to read:

- Read `Tensorflow: a system for large-scale machine learning <https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf>`_, Abadi, Mart√≠n, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin et al. OSDI, vol. 16, pp. 265-283. 2016.

- Read `Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis <https://arxiv.org/pdf/1802.09941.pdf>`_, Ben-Nun and Hoefler.

**Wednesday, October 24**: `Compilation <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture8-Compilation.pdf>`_ (slides download).


----

Week 5 - Science and Performance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, October 29**: `Machine Learning and Science, and the Data and Learning Hub <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture9-Materials.pdf>`_ (slides download).

- Guest lecture: Ben Blaiszik, Ryan Chard, and Logan Ward

**Wednesday, October 31**: `Measuring and Understanding Performance <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture10-Performance.pdf>`_ (slides download).

Papers to read:

- `DAWNBench: An End-to-End Deep Learning Benchmark and Competition <https://dawn.cs.stanford.edu/benchmark/papers/nips17-dawnbench.pdf>`_, Cody Coleman et al., NIPS 2017. Also `DAWNBench web site <https://dawn.cs.stanford.edu/benchmark/>`_ and `Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark <https://arxiv.org/pdf/1806.01427.pdf>`_, Cody Coleman et al., Arxiv.
- `MLPerf <https://mlperf.org>`_

----

Week 6 - From Edge to Supercomputers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, November 5**: `Urban Sensing and Edge Computing (Charlie Catlett) <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/CeC-UChicago-5Nov2018-ilovepdf-compressed.pdf>`_ (slides download).

**Wednesday, November 7**: `Deep Learning at Scale (Venkat Vishwanath) <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/VIshwanath_UCHICAGO_LearningCLASS.pdf>`_ (slides download).

----

Week 7 - Silicon Trends and AutoML
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, November 12**: `Silicon trends <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture14-Silicon-Trends-DL-compressed.pdf>`_ (slides download).

**Wednesday, November 14**: `AutoML <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture15-AutoML-compressed.pdf>`_ (slides download).



----

Week 8 - Parallelism
~~~~~~~~~~~~~~~~~~~~

**Monday, November 19**: `Parallelism <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture16-Parallelism.pdf>`_ (slides download).

**Wednesday, November 21**

- TBD

----

Week 9 - TOPIC TBD
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, November 26**: `Zhao Zhang, TACC <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture17-Zhang.pdf>`_ (slides download).

**Wednesday, November 28**: `Nicholas Malaya, AMD <https://github.com/uchicago-cs/cmsc35200/raw/master/resources/Lecture18-Malaya.pdf>`_ (slides download).


----

Week 10 - Project presentations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Monday, December 3**

- Project presentations

**Wednesday, December 5**

- Project presentations

----

Specific topics to be covered:

* Introduction to deep learning models
* Functional content of deep learning frameworks
* Software architecture and design of frameworks
* Abstraction layers for deep learning
* Performance and benchmarking deep learning systems
* Hardware architectures for accelerating deep learning. E.g., `Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks <http://cadlab.cs.ucla.edu/~cong/slides/fpga2015_chen.pdf>`_, Chen Zhang et al,
* Parallelism (model, data, ensemble). E>g., `Large Scale Distributed Deep Networks <http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf>`_, Jeff Dean et al., NIPS 2012.
* Portable representations and translations of models
* Optimization for training, inference
* Workflows for machine learning and workflow tools
* Hyper-parameter optimization and ensembles
* Uncertainty quantification

Other potential topics:

* Compression of networks. E.g., `Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding <https://arxiv.org/pdf/1510.00149.pdf>`_, Song Han et al, ICLR 2016.
* I/O
